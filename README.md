# Run Large Language Model on CPU

#### llama-cpp

**Mistral_7B_DPO_Pandas-q8_0.gguf** provides this response for the financial task on the local. This GGUF model is 8-bit quantization(q8_0) 
![Screenshot 2025-01-30 105558](https://github.com/user-attachments/assets/5806438f-aedd-46f8-b656-6ca5860fd6d7)


## Download the GGUF models
Hugging Face: https://huggingface.co/nirusanan/llama-gguf
