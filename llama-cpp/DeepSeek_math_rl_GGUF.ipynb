{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TuC0aTWpWQr"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"transformers==4.38.1\" \"datasets==2.13.0\" \"peft==0.6.2\" \"accelerate==0.21.0\" \"bitsandbytes==0.41.2.post2\" \"trl==0.4.7\" \"safetensors>=0.3.1\" \"gradio\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp.git"
      ],
      "metadata": {
        "id": "vn4-hjQkpWrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r llama.cpp/requirements.txt"
      ],
      "metadata": {
        "id": "TiS7N3KMpWuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "model_name = \"deepseek-ai/deepseek-math-7b-rl\"\n",
        "\n",
        "snapshot_download(repo_id=model_name, local_dir=\"deep-seek-math-rl\",\n",
        "                  local_dir_use_symlinks=False, revision=\"main\")"
      ],
      "metadata": {
        "id": "DZk0oIzMpWw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/llama.cpp/convert_hf_to_gguf.py deep-seek-math-rl \\\n",
        "  --outfile deepseek-math-rl.gguf \\\n",
        "  --outtype q8_0"
      ],
      "metadata": {
        "id": "0KEDSzDTpW2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUdFrXK6AAUP",
        "outputId": "04953b1a-fa6b-44c2-b9d1-9920e7d49807"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(model_path=\"/content/deepseek-math-rl.gguf\", chat_format=\"llama-2\",n_ctx=850, n_gpu_layers=30)"
      ],
      "metadata": {
        "id": "oNvdc_JbpW8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "320d2f48-faba-410d-cf22-ff6917cf0165"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 29 key-value pairs and 273 tensors from /content/deepseek-math-rl.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Deep Seek Math Rl\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 6.9B\n",
            "llama_model_loader: - kv   4:                            general.license str              = other\n",
            "llama_model_loader: - kv   5:                       general.license.name str              = deepseek\n",
            "llama_model_loader: - kv   6:                       general.license.link str              = https://github.com/deepseek-ai/DeepSe...\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 30\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  15:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  16:                           llama.vocab_size u32              = 102400\n",
            "llama_model_loader: - kv  17:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = deepseek-llm\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,102400]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,99757]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"i n\", \"h e...\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 100000\n",
            "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 100001\n",
            "llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   61 tensors\n",
            "llama_model_loader: - type q8_0:  212 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q8_0\n",
            "print_info: file size   = 6.84 GiB (8.50 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 100001 '<｜end▁of▁sentence｜>' is not marked as EOG\n",
            "load: control token: 100000 '<｜begin▁of▁sentence｜>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 2\n",
            "load: token to piece cache size = 0.6408 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 30\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 4096\n",
            "print_info: n_embd_v_gqa     = 4096\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: n_ff             = 11008\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = ?B\n",
            "print_info: model params     = 6.91 B\n",
            "print_info: general.name     = Deep Seek Math Rl\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 102400\n",
            "print_info: n_merges         = 99757\n",
            "print_info: BOS token        = 100000 '<｜begin▁of▁sentence｜>'\n",
            "print_info: EOS token        = 100001 '<｜end▁of▁sentence｜>'\n",
            "print_info: EOT token        = 100001 '<｜end▁of▁sentence｜>'\n",
            "print_info: LF token         = 126 'Ä'\n",
            "print_info: EOG token        = 100001 '<｜end▁of▁sentence｜>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: layer  29 assigned to device CPU\n",
            "load_tensors: layer  30 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q8_0) (and 272 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  7002.83 MiB\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 864\n",
            "llama_init_from_model: n_ctx_per_seq = 864\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (864) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 864, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 30, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\n",
            "llama_kv_cache_init:        CPU KV buffer size =   405.00 MiB\n",
            "llama_init_from_model: KV self size  =  405.00 MiB, K (f16):  202.50 MiB, V (f16):  202.50 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.39 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   208.00 MiB\n",
            "llama_init_from_model: graph nodes  = 966\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.eos_token_id': '100001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '102400', 'general.file_type': '7', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.freq_base': '10000.000000', 'general.architecture': 'llama', 'tokenizer.ggml.bos_token_id': '100000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'deepseek-llm', 'llama.context_length': '4096', 'general.name': 'Deep Seek Math Rl', 'general.type': 'model', 'general.size_label': '6.9B', 'general.license.name': 'deepseek', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ message['content'] + '\\n\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\", 'general.license.link': 'https://github.com/deepseek-ai/DeepSeek-Math/blob/main/LICENSE-MODEL', 'general.license': 'other', 'llama.feed_forward_length': '11008', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.block_count': '30', 'llama.attention.head_count_kv': '32'}\n",
            "Available chat formats from metadata: chat_template.default\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "\n",
        "How large should we take $n$ in order to guarantee that the Trapezoidal and Midpoint Rule approximations for $\\\\int_1^2(1 / x) d x$ are accurate to within 0.0001 ?\\nPlease reason step by step, and put your final answer within \\\\boxed{}."
      ],
      "metadata": {
        "id": "C6uHOue7JKyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"How large should we take $n$ in order to guarantee that the Trapezoidal and Midpoint Rule approximations for $\\\\int_1^2(1 / x) d x$ are accurate to within 0.0001 ?\\nPlease reason step by step, and put your final answer within \\\\boxed{}.\"\"\"\n",
        "\n",
        "output = llm(\n",
        "  prompt,\n",
        "  max_tokens=850,\n",
        "  echo=True\n",
        ")\n",
        "\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "id": "59y4xmDppXB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3552d46-bf3c-4afa-82f5-23ac5934dd83"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   17522.35 ms\n",
            "llama_perf_context_print: prompt eval time =   17522.04 ms /    67 tokens (  261.52 ms per token,     3.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =  702073.59 ms /   706 runs   (  994.44 ms per token,     1.01 tokens per second)\n",
            "llama_perf_context_print:       total time =  720835.12 ms /   773 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How large should we take $n$ in order to guarantee that the Trapezoidal and Midpoint Rule approximations for $\\int_1^2(1 / x) d x$ are accurate to within 0.0001 ?\n",
            "Please reason step by step, and put your final answer within \\boxed{}.\n",
            "To find the value of $n$ that guarantees the accuracy of the Trapezoidal and Midpoint Rule approximations for $\\int_1^2 \\frac{1}{x} dx$, we need to find the error bounds for these methods.\n",
            "For the Trapezoidal Rule, the error bound is given by $\\frac{(b-a)^3}{12n^2}M$, where $M$ is the maximum value of the second derivative of the function on the interval $[a, b]$.\n",
            "For the function $f(x) = \\frac{1}{x}$, the first derivative is $f'(x) = -\\frac{1}{x^2}$ and the second derivative is $f''(x) = \\frac{2}{x^3}$.\n",
            "The maximum value of the second derivative on the interval $[1, 2]$ is $f''(1) = 2$.\n",
            "Thus, the error bound for the Trapezoidal Rule is $\\frac{(2-1)^3}{12n^2} \\cdot 2 = \\frac{2}{12n^2} = \\frac{1}{6n^2}$.\n",
            "\n",
            "For the Midpoint Rule, the error bound is given by $\\frac{(b-a)^3}{24n^3}M'$, where $M'$ is the maximum value of the third derivative of the function on the interval $[a, b]$.\n",
            "The third derivative of $f(x) = \\frac{1}{x}$ is $f'''(x) = -\\frac{6}{x^4}$.\n",
            "The maximum value of the third derivative on the interval $[1, 2]$ is $f'''(1) = -6$.\n",
            "Thus, the error bound for the Midpoint Rule is $\\frac{(2-1)^3}{24n^3} \\cdot 6 = \\frac{6}{24n^3} = \\frac{1}{4n^3}$.\n",
            "\n",
            "We want the error bounds to be less than 0.0001, so we set up the inequalities:\n",
            "$\\frac{1}{6n^2} \\leq 0.0001$ and $\\frac{1}{4n^3} \\leq 0.0001$.\n",
            "\n",
            "Solving the first inequality, we have $n^2 \\geq \\frac{1}{0.0006}$, so $n \\geq \\sqrt{\\frac{1}{0.0006}} \\approx 40.8$. Since $n$ must be an integer, we take the smallest integer greater than 40.8, which is 41.\n",
            "\n",
            "Solving the second inequality, we have $n^3 \\geq \\frac{1}{0.0004}$, so $n \\geq \\sqrt[3]{\\frac{1}{0.0004}} \\approx 62.9$. Since $n$ must be an integer, we take the smallest integer greater than 62.9, which is 63.\n",
            "\n",
            "Since we want the smallest value of $n$ that guarantees the accuracy, we take the maximum of 41 and 63, which is 63.\n",
            "Therefore, we should take $n = 63$. The answer is: $63$\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "To find the value of $n$ that guarantees the accuracy of the Trapezoidal and Midpoint Rule approximations for $\\int_1^2 \\frac{1}{x} dx$, we need to find the error bounds for these methods.\n",
        "For the Trapezoidal Rule, the error bound is given by $\\frac{(b-a)^3}{12n^2}M$, where $M$ is the maximum value of the second derivative of the function on the interval $[a, b]$.\n",
        "For the function $f(x) = \\frac{1}{x}$, the first derivative is $f'(x) = -\\frac{1}{x^2}$ and the second derivative is $f''(x) = \\frac{2}{x^3}$.\n",
        "The maximum value of the second derivative on the interval $[1, 2]$ is $f''(1) = 2$.\n",
        "Thus, the error bound for the Trapezoidal Rule is $\\frac{(2-1)^3}{12n^2} \\cdot 2 = \\frac{2}{12n^2} = \\frac{1}{6n^2}$.\n",
        "\n",
        "For the Midpoint Rule, the error bound is given by $\\frac{(b-a)^3}{24n^3}M'$, where $M'$ is the maximum value of the third derivative of the function on the interval $[a, b]$.\n",
        "The third derivative of $f(x) = \\frac{1}{x}$ is $f'''(x) = -\\frac{6}{x^4}$.\n",
        "The maximum value of the third derivative on the interval $[1, 2]$ is $f'''(1) = -6$.\n",
        "Thus, the error bound for the Midpoint Rule is $\\frac{(2-1)^3}{24n^3} \\cdot 6 = \\frac{6}{24n^3} = \\frac{1}{4n^3}$.\n",
        "\n",
        "We want the error bounds to be less than 0.0001, so we set up the inequalities:\n",
        "$\\frac{1}{6n^2} \\leq 0.0001$ and $\\frac{1}{4n^3} \\leq 0.0001$.\n",
        "\n",
        "Solving the first inequality, we have $n^2 \\geq \\frac{1}{0.0006}$, so $n \\geq \\sqrt{\\frac{1}{0.0006}} \\approx 40.8$. Since $n$ must be an integer, we take the smallest integer greater than 40.8, which is 41.\n",
        "\n",
        "Solving the second inequality, we have $n^3 \\geq \\frac{1}{0.0004}$, so $n \\geq \\sqrt[3]{\\frac{1}{0.0004}} \\approx 62.9$. Since $n$ must be an integer, we take the smallest integer greater than 62.9, which is 63.\n",
        "\n",
        "Since we want the smallest value of $n$ that guarantees the accuracy, we take the maximum of 41 and 63, which is 63.\n",
        "Therefore, we should take $n = 63$. The answer is: $63$"
      ],
      "metadata": {
        "id": "JCFKJBzRJZI-"
      }
    }
  ]
}